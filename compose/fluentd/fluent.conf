# ============================================================
# fluentd/fluent.conf
#
# Flow:
#   Docker log driver (port 24224)
#     → parse inner JSON log field
#     → enrich with service metadata
#     → [airflow.**]  → MinIO S3 bucket  +  stdout
#     → [**]          → stdout only (catch-all for other services)
#
# Tags set via docker-compose logging.tag:
#   airflow.scheduler
#   airflow.api-server
#   airflow.init
# ============================================================

# ── Input ────────────────────────────────────────────────────
# Receive logs forwarded by Docker's fluentd log driver
<source>
  @type    forward
  @id      in_docker_forward
  port     24224
  bind     0.0.0.0
</source>

# ── Filters ──────────────────────────────────────────────────

# 1. Parse the inner `log` field as JSON (Airflow 3 task logs are
#    already JSON). Falls back gracefully to plain text via
#    multi-format-parser so non-JSON scheduler lines don't break.
<filter airflow.**>
  @type    parser
  @id      parse_airflow_json
  key_name log
  reserve_data  true   # keep container_id, container_name etc.
  emit_invalid_record_to_error false   # silently skip unparseable lines

  <parse>
    @type multi_format

    # Airflow 3 structured task log
    <pattern>
      format      json
      time_key    timestamp
      time_format %Y-%m-%dT%H:%M:%S.%L%z
    </pattern>

    # Plain text fallback (scheduler startup lines, etc.)
    <pattern>
      format none
    </pattern>
  </parse>
</filter>

# 2. Add a human-readable `service` field from the tag
#    e.g. tag=airflow.scheduler → service=scheduler
<filter airflow.**>
  @type         record_transformer
  @id           enrich_service_field
  enable_ruby   true
  <record>
    service     ${tag_suffix[1]}   # "scheduler" / "api-server" / "init"
    environment "#{ENV['AIRFLOW_VAR_ENVIRONMENT'] || 'local'}"
    host        "#{Socket.gethostname}"
  </record>
</filter>

# ── Outputs ──────────────────────────────────────────────────

# Airflow logs → MinIO S3 bucket + stdout
<match airflow.**>
  @type  copy
  @id    out_airflow

  # ── MinIO / S3 sink ──────────────────────────────────────
  <store>
    @type           s3
    @id             out_s3_airflow

    aws_key_id      "#{ENV['MINIO_ROOT_USER']}"
    aws_sec_key     "#{ENV['MINIO_ROOT_PASSWORD']}"
    s3_bucket       airflow-logs
    s3_region       us-east-1
    s3_endpoint     "http://#{ENV['MINIO_HOST'] || 'minio'}:9000"
    force_path_style  true          # required for MinIO path-style access
    store_as          json          # store log files as .json

    # Partition by service and date for easy querying with Trino/Iceberg
    path              logs/${service}/%Y/%m/%d/
    s3_object_key_format  %{path}%{time_slice}_%{index}.%{file_extension}
    time_slice_format     %H%M       # one file per hour per service

    <format>
      @type json
    </format>

    <buffer service,time>
      @type              file
      path               /fluentd/buffer/s3
      dirmode            0755
      filemode           0644
      timekey            3600          # flush every 1 hour
      timekey_wait       10m           # wait 10 min after hour boundary
      chunk_limit_size   256m
      flush_thread_count 4
      overflow_action    block
      retry_max_times    5
      retry_wait         30s
    </buffer>
  </store>

  # ── Stdout sink (local debug) ─────────────────────────────
  <store>
    @type  stdout
    @id    out_stdout_airflow
    <format>
      @type json
    </format>
  </store>
</match>

# Catch-all: any other container logs → stdout only
<match **>
  @type  stdout
  @id    out_stdout_default
  <format>
    @type json
  </format>
</match>